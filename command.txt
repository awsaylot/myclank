Use this to start
docker run --gpus all --rm -it -v ${PWD}:/models -p 8080:8080 ghcr.io/ggml-org/llama.cpp:full-cuda -s -m /models/CodeLlama-7b-Instruct-hf.Q6_K.gguf --port 8080 --host 0.0.0.0

Use this to test
curl -X POST http://0.0.0.0:8080/v1/chat/completions -H "Content-Type: application/json" -d '{"model":"CodeLlama-7b-Instruct-hf.Q6_K.gguf","messages":[{"role":"user","content":"Once upon a time,"}],"max_tokens":128}'